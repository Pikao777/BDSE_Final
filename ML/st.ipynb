{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37585882",
   "metadata": {},
   "source": [
    "### Tokenizer 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb2c0300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-jd-full-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"star 1\",\n",
      "    \"1\": \"star 2\",\n",
      "    \"2\": \"star 3\",\n",
      "    \"3\": \"star 4\",\n",
      "    \"4\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": 0,\n",
      "    \"star 2\": 1,\n",
      "    \"star 3\": 2,\n",
      "    \"star 4\": 3,\n",
      "    \"star 5\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-jd-full-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"star 1\",\n",
      "    \"1\": \"star 2\",\n",
      "    \"2\": \"star 3\",\n",
      "    \"3\": \"star 4\",\n",
      "    \"4\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": 0,\n",
      "    \"star 2\": 1,\n",
      "    \"star 3\": 2,\n",
      "    \"star 4\": 3,\n",
      "    \"star 5\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-jd-full-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"star 1\",\n",
      "    \"1\": \"star 2\",\n",
      "    \"2\": \"star 3\",\n",
      "    \"3\": \"star 4\",\n",
      "    \"4\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": 0,\n",
      "    \"star 2\": 1,\n",
      "    \"star 3\": 2,\n",
      "    \"star 4\": 3,\n",
      "    \"star 5\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = \"uer/roberta-base-finetuned-jd-full-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3bf7bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7614, 3160, 2523, 1962, 1600,  102,    0,    0,    0,    0],\n",
      "        [ 101, 4472, 1862, 2523, 7766, 8024, 6917, 3300, 5439, 7962,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "string_arr = [\n",
    "    \"飲料很好喝\",\n",
    "    \"環境很髒，還有老鼠\"\n",
    "]\n",
    "inputs = tokenizer(string_arr, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bace2d3",
   "metadata": {},
   "source": [
    "### Transformers model使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a1584d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"star 1\",\n",
      "    \"1\": \"star 2\",\n",
      "    \"2\": \"star 3\",\n",
      "    \"3\": \"star 4\",\n",
      "    \"4\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": 0,\n",
      "    \"star 2\": 1,\n",
      "    \"star 3\": 2,\n",
      "    \"star 4\": 3,\n",
      "    \"star 5\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at uer/roberta-base-finetuned-jd-full-chinese were not used when initializing RobertaForSequenceClassification: ['bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'classifier.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'classifier.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at uer/roberta-base-finetuned-jd-full-chinese and are newly initialized: ['encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.weight', 'classifier.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'classifier.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'classifier.out_proj.weight', 'encoder.layer.3.output.dense.weight', 'classifier.out_proj.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "model_name = \"uer/roberta-base-finetuned-jd-full-chinese\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1d13dd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1647,  0.1126, -0.0676, -0.2357, -0.2487],\n",
      "        [ 0.1112,  0.0685, -0.1857, -0.1873, -0.2324]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print output's logits\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f3c1d",
   "metadata": {},
   "source": [
    "### 將結果過 softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "caf1e6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2455, 0.2330, 0.1946, 0.1645, 0.1624],\n",
      "        [0.2408, 0.2308, 0.1790, 0.1787, 0.1708]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ed389",
   "metadata": {},
   "source": [
    "### 查看model label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cbbdc1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'star 1', 1: 'star 2', 2: 'star 3', 3: 'star 4', 4: 'star 5'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1858a33",
   "metadata": {},
   "source": [
    "### 以上程式碼簡化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eb7ebcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-jd-full-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"star 1\",\n",
      "    \"1\": \"star 2\",\n",
      "    \"2\": \"star 3\",\n",
      "    \"3\": \"star 4\",\n",
      "    \"4\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": 0,\n",
      "    \"star 2\": 1,\n",
      "    \"star 3\": 2,\n",
      "    \"star 4\": 3,\n",
      "    \"star 5\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at uer/roberta-base-finetuned-jd-full-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'star 5', 'score': 0.3979003429412842},\n",
       " {'label': 'star 2', 'score': 0.37679576873779297}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_name = \"uer/roberta-base-finetuned-jd-full-chinese\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\n",
    "    [\n",
    "    \"飲料很好喝\",\n",
    "    \"環境很髒，還有老鼠\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a36e9",
   "metadata": {},
   "source": [
    "# Tensorflow 模型加載"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "58d23393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-jd-full-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"star 1\",\n",
      "    \"1\": \"star 2\",\n",
      "    \"2\": \"star 3\",\n",
      "    \"3\": \"star 4\",\n",
      "    \"4\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": 0,\n",
      "    \"star 2\": 1,\n",
      "    \"star 3\": 2,\n",
      "    \"star 4\": 3,\n",
      "    \"star 5\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file tf_model.h5 from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\tf_model.h5\n",
      "Some layers from the model checkpoint at uer/roberta-base-finetuned-jd-full-chinese were not used when initializing TFBertModel: ['classifier', 'dropout_37']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at uer/roberta-base-finetuned-jd-full-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "tf_model = TFAutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fd701",
   "metadata": {},
   "source": [
    "### 載入dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "84c99b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "70427be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict0 = {\n",
    "    1:0,\n",
    "    2:1,\n",
    "    3:2,\n",
    "    4:3,\n",
    "    5:4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0bdfaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df = train_df.drop(columns=['Unnamed: 0'])\n",
    "train_df = train_df[train_df['verse_text'].str.len() < 500]\n",
    "train_df['label'] = train_df['label'].map(dict0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75fcc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df = test_df.drop(columns=['Unnamed: 0'])\n",
    "test_df = test_df[test_df['verse_text'].str.len() < 500]\n",
    "test_df['label'] = test_df['label'].map(dict0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "22c4c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv('validation.csv')\n",
    "validation_df = validation_df.drop(columns=['Unnamed: 0'])\n",
    "validation_df = validation_df[validation_df['verse_text'].str.len() < 500]\n",
    "validation_df['label'] = validation_df['label'].map(dict0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dad1cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "# my_dataset_dict = datasets.DatasetDict({\"train\":train_dataset,\"validation\":validation_dataset ,\"test\":test_dataset})\n",
    "my_dataset_dict = DatasetDict({\"train\":train_dataset,\"validation\":validation_dataset ,\"test\":test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ce7155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['verse_text', 'label', '__index_level_0__'],\n",
       "        num_rows: 132\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['verse_text', 'label', '__index_level_0__'],\n",
       "        num_rows: 101\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['verse_text', 'label', '__index_level_0__'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "32b5aa41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verse_text</th>\n",
       "      <th>label</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>純素鹽酥雞很好吃！猴頭菇的口感很紮實\\n\\n</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中規中矩，水果沙拉水果很多令人印象深刻\\n\\n</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>東西好吃\\n\\n</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>很喜歡餐點好吃\\n\\n</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>義大利麵醬汁很濃郁，堅果類給蠻多，但菇的存在感有點低~臭豆腐麵所謂的拉麵有點像泡麵，跟預期不...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          verse_text  label  __index_level_0__\n",
       "0                             純素鹽酥雞很好吃！猴頭菇的口感很紮實\\n\\n      4                  0\n",
       "1                            中規中矩，水果沙拉水果很多令人印象深刻\\n\\n      3                  1\n",
       "2                                           東西好吃\\n\\n      4                  2\n",
       "3                                        很喜歡餐點好吃\\n\\n      4                  3\n",
       "4  義大利麵醬汁很濃郁，堅果類給蠻多，但菇的存在感有點低~臭豆腐麵所謂的拉麵有點像泡麵，跟預期不...      2                  4"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset_dict.set_format(type=\"pandas\")\n",
    "df = my_dataset_dict[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d886ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"star 1\",\"star 2\",\"star 3\",\"star 4\",\"star 5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "09d0421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset_dict.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bff72",
   "metadata": {},
   "source": [
    "### 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "688f8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-jd-full-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"star 1\",\n",
      "    \"1\": \"star 2\",\n",
      "    \"2\": \"star 3\",\n",
      "    \"3\": \"star 4\",\n",
      "    \"4\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": 0,\n",
      "    \"star 2\": 1,\n",
      "    \"star 3\": 2,\n",
      "    \"star 4\": 3,\n",
      "    \"star 5\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-jd-full-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"star 1\",\n",
      "    \"1\": \"star 2\",\n",
      "    \"2\": \"star 3\",\n",
      "    \"3\": \"star 4\",\n",
      "    \"4\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": 0,\n",
      "    \"star 2\": 1,\n",
      "    \"star 3\": 2,\n",
      "    \"star 4\": 3,\n",
      "    \"star 5\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-jd-full-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"star 1\",\n",
      "    \"1\": \"star 2\",\n",
      "    \"2\": \"star 3\",\n",
      "    \"3\": \"star 4\",\n",
      "    \"4\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": 0,\n",
      "    \"star 2\": 1,\n",
      "    \"star 3\": 2,\n",
      "    \"star 4\": 3,\n",
      "    \"star 5\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"uer/roberta-base-finetuned-jd-full-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "65572d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"verse_text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0dc31678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.48ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 47.61ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.01ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verse_text': '純素鹽酥雞很好吃！猴頭菇的口感很紮實\\n\\n',\n",
       " 'label': 4,\n",
       " '__index_level_0__': 0,\n",
       " 'input_ids': [101,\n",
       "  5155,\n",
       "  5162,\n",
       "  7921,\n",
       "  6989,\n",
       "  7430,\n",
       "  2523,\n",
       "  1962,\n",
       "  1391,\n",
       "  8013,\n",
       "  4347,\n",
       "  7531,\n",
       "  5823,\n",
       "  4638,\n",
       "  1366,\n",
       "  2697,\n",
       "  2523,\n",
       "  5167,\n",
       "  2179,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset_dict_encoded = my_dataset_dict.map(tokenize, batched=True, batch_size=None)\n",
    "next(iter(my_dataset_dict_encoded[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bb223288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds = my_dataset_dict[\"validation\"]\n",
    "valid_ds[\"label\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652eff3",
   "metadata": {},
   "source": [
    "### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9d7f46a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-jd-full-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"1\": \"star 1\",\n",
      "    \"2\": \"star 2\",\n",
      "    \"3\": \"star 3\",\n",
      "    \"4\": \"star 4\",\n",
      "    \"5\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": \"1\",\n",
      "    \"star 2\": \"2\",\n",
      "    \"star 3\": \"3\",\n",
      "    \"star 4\": \"4\",\n",
      "    \"star 5\": \"5\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\jimmy/.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-jd-full-chinese\\snapshots\\001c14a6ad8498465b0d7a2be435c30e856507a8\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at uer/roberta-base-finetuned-jd-full-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "num_labels = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = (AutoModelForSequenceClassification\n",
    "        .from_pretrained(model_name, num_labels=num_labels\n",
    "        ,id2label={\"1\": \"star 1\",\n",
    "                    \"2\": \"star 2\",\n",
    "                    \"3\": \"star 3\",\n",
    "                    \"4\": \"star 4\",\n",
    "                    \"5\": \"star 5\"}\n",
    "        ,label2id={\"star 1\": \"1\",\n",
    "                    \"star 2\": \"2\",\n",
    "                    \"star 3\": \"3\",\n",
    "                    \"star 4\": \"4\",\n",
    "                    \"star 5\": \"5\" })\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e2aad484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "logging_steps = len(my_dataset_dict_encoded[\"train\"]) // batch_size\n",
    "model_name = \"test_model\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=1,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  label_names= labels,\n",
    "                                  report_to = \"mlflow\",\n",
    "                                  logging_steps=logging_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5701d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f57f3d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, verse_text. If __index_level_0__, verse_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 132\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5\n",
      "  Number of trainable parameters = 102271493\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 05:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, verse_text. If __index_level_0__, verse_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=0.5449041545391082, metrics={'train_runtime': 407.7195, 'train_samples_per_second': 0.324, 'train_steps_per_second': 0.012, 'total_flos': 22724773944840.0, 'train_loss': 0.5449041545391082, 'epoch': 1.0})"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=my_dataset_dict_encoded[\"train\"],\n",
    "                  eval_dataset=my_dataset_dict_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "59b76f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, verse_text. If __index_level_0__, verse_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(my_dataset_dict_encoded[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "80798d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([0.6265982 , 0.52048177, 0.45936096, 0.38796163], dtype=float32), array([[-2.7288473e+00, -7.6547980e-01,  9.1133302e-01,  1.2757695e+00,\n",
      "        -8.0046207e-02],\n",
      "       [-3.1789901e+00, -2.0193393e+00,  1.6839665e-01,  2.7926512e+00,\n",
      "         2.2123790e+00],\n",
      "       [-2.4288175e+00, -6.5146160e-01,  2.7815497e-01,  7.2900718e-01,\n",
      "         4.5314136e-01],\n",
      "       [-2.2955234e+00, -1.6273518e+00, -8.8209504e-01,  1.1679218e+00,\n",
      "         3.5461721e+00],\n",
      "       [-1.8185469e+00, -1.1070533e-02,  8.0031931e-01,  3.6078274e-01,\n",
      "        -8.4036911e-01],\n",
      "       [-2.4904029e+00, -1.0933578e+00, -1.5280648e-01,  9.0126085e-01,\n",
      "         1.4588159e+00],\n",
      "       [-2.5208831e+00, -2.1357288e+00, -5.2324647e-01,  2.2614858e+00,\n",
      "         3.1605501e+00],\n",
      "       [-2.3408725e+00, -4.7879145e-01,  7.5424808e-01,  1.1433117e+00,\n",
      "        -3.8963002e-01],\n",
      "       [-1.9497659e+00, -1.7209349e+00, -1.0178092e+00,  1.1927373e+00,\n",
      "         3.3378363e+00],\n",
      "       [-2.1956036e+00, -1.6090763e+00, -9.1733104e-01,  1.0311711e+00,\n",
      "         3.4411693e+00],\n",
      "       [-2.6397426e+00, -1.9830234e+00, -4.1155547e-01,  2.1785483e+00,\n",
      "         2.9293690e+00],\n",
      "       [-2.6180866e+00, -1.7340323e+00, -3.3446372e-01,  1.8483627e+00,\n",
      "         2.3509324e+00],\n",
      "       [-2.6556880e+00, -1.2676179e+00, -2.1564519e-01,  9.7595108e-01,\n",
      "         2.1265814e+00],\n",
      "       [-2.5020227e+00, -1.7973272e+00, -1.6958717e-01,  1.8145139e+00,\n",
      "         1.9626794e+00],\n",
      "       [-2.3822289e+00, -1.3916863e+00, -3.2331371e-01,  1.2038641e+00,\n",
      "         1.8861817e+00],\n",
      "       [-2.9450786e+00, -2.2050626e+00, -4.7128253e-02,  2.6829467e+00,\n",
      "         2.7770550e+00],\n",
      "       [-1.7515152e+00, -1.6277398e+00, -1.1703248e+00,  7.3591685e-01,\n",
      "         3.2952385e+00],\n",
      "       [-1.7825146e+00, -8.3322227e-01, -3.7538409e-01,  2.6418635e-01,\n",
      "         9.0898675e-01],\n",
      "       [-2.8757832e+00, -1.4458187e+00,  5.6745194e-02,  1.5841780e+00,\n",
      "         1.6479447e+00],\n",
      "       [-2.7998121e+00, -1.5958698e+00, -9.8444164e-02,  1.7826054e+00,\n",
      "         1.9963639e+00],\n",
      "       [-1.7706052e+00, -2.2616150e+00, -1.2489876e+00,  1.9570338e+00,\n",
      "         3.9650042e+00],\n",
      "       [-2.6829922e+00, -1.3442345e+00, -2.3122244e-01,  1.1603259e+00,\n",
      "         2.2109969e+00],\n",
      "       [-5.2571398e-01, -8.8351816e-01, -8.6256307e-01, -3.3984655e-01,\n",
      "         5.0262237e-01],\n",
      "       [-2.1934428e+00, -1.5030153e+00, -5.4439324e-01,  1.0930785e+00,\n",
      "         2.3125780e+00],\n",
      "       [-1.7188938e+00, -8.7158751e-01, -4.4041610e-01,  2.1509725e-01,\n",
      "         9.9380541e-01],\n",
      "       [-2.6624916e+00, -1.6632737e+00, -1.9662459e-01,  1.6326706e+00,\n",
      "         2.1810255e+00],\n",
      "       [-2.0323305e+00, -1.3232967e+00, -8.8473481e-01,  5.1179320e-01,\n",
      "         2.8576865e+00],\n",
      "       [-2.5093589e+00, -1.3003651e+00, -2.3593481e-01,  1.1882370e+00,\n",
      "         1.7966284e+00],\n",
      "       [-1.9187653e+00, -1.3927649e+00, -9.6312839e-01,  5.5305409e-01,\n",
      "         2.9254339e+00],\n",
      "       [-1.9322083e+00, -1.2719052e+00, -8.3405656e-01,  3.8472098e-01,\n",
      "         2.5541084e+00],\n",
      "       [-1.5226860e+00, -1.1298029e+00, -1.1056490e+00, -6.0611133e-02,\n",
      "         2.5915036e+00],\n",
      "       [-1.7265632e+00, -1.2208455e+00, -8.9039904e-01,  2.4071592e-01,\n",
      "         2.2484286e+00],\n",
      "       [-1.8841989e+00, -1.7713190e+00, -1.2266510e+00,  1.1893494e+00,\n",
      "         3.7352242e+00],\n",
      "       [-2.6433232e+00, -1.8750232e+00, -3.1260079e-01,  1.8975739e+00,\n",
      "         2.5920219e+00],\n",
      "       [-2.7590706e+00, -1.6374140e+00, -1.5963805e-01,  1.7067298e+00,\n",
      "         2.1438611e+00],\n",
      "       [-2.0450900e+00, -6.6690350e-01, -1.8248309e-01,  2.1300301e-01,\n",
      "         8.4563488e-01],\n",
      "       [-2.6584461e+00, -1.9801415e+00, -4.3258125e-01,  2.4291344e+00,\n",
      "         3.0703871e+00],\n",
      "       [-2.6701207e+00, -1.6994271e+00, -4.2235988e-01,  1.7872585e+00,\n",
      "         2.7968636e+00],\n",
      "       [-2.9022989e+00, -1.7381322e+00,  4.5313883e-01,  2.5846963e+00,\n",
      "         1.4236929e+00],\n",
      "       [-2.2115560e+00, -1.6088779e+00, -6.8793029e-01,  1.1347017e+00,\n",
      "         2.8227186e+00],\n",
      "       [-1.3010938e+00, -1.1235011e+00,  9.4540350e-02,  3.7747989e+00,\n",
      "        -5.7974517e-01],\n",
      "       [-1.8668166e+00, -1.1492448e+00, -8.5720056e-01,  1.4041698e-01,\n",
      "         2.4734616e+00],\n",
      "       [-1.9177673e+00, -1.8165731e+00, -1.0647330e+00,  1.1498430e+00,\n",
      "         3.5606606e+00],\n",
      "       [-3.1655707e+00, -1.7542354e+00,  2.0100158e-01,  2.2294154e+00,\n",
      "         1.9431279e+00],\n",
      "       [-2.1038535e+00, -6.5063447e-01, -2.6626989e-02,  3.9380488e-01,\n",
      "         6.1331475e-01],\n",
      "       [-2.5704637e+00, -2.0649853e+00, -4.1564667e-01,  2.4339051e+00,\n",
      "         2.8199077e+00],\n",
      "       [-1.9071131e+00, -1.7224839e+00, -1.1469374e+00,  1.1905503e+00,\n",
      "         3.5636034e+00],\n",
      "       [-2.3360054e+00, -1.9443172e+00, -7.2144407e-01,  1.9521036e+00,\n",
      "         3.0302601e+00],\n",
      "       [-2.7527807e+00, -1.8215845e+00, -2.8640908e-01,  1.9622263e+00,\n",
      "         2.6540284e+00],\n",
      "       [-2.2935107e+00, -1.8500514e+00, -6.3518715e-01,  1.6953523e+00,\n",
      "         2.8236012e+00],\n",
      "       [-2.8264620e+00, -1.8695269e+00, -2.3530583e-01,  2.2413104e+00,\n",
      "         2.5397005e+00],\n",
      "       [-2.0374172e+00, -1.6135211e+00, -1.0500278e+00,  9.6535397e-01,\n",
      "         3.5200791e+00],\n",
      "       [-2.4281843e+00, -1.8131518e+00,  2.4515167e-03,  1.6365852e+00,\n",
      "         1.7674154e+00],\n",
      "       [-1.9933387e+00, -1.6377453e+00, -7.8841525e-01,  1.1074258e+00,\n",
      "         2.6428771e+00],\n",
      "       [-2.6409385e+00, -1.5775510e+00, -2.8111213e-01,  1.4885658e+00,\n",
      "         2.2950077e+00],\n",
      "       [-1.8938957e+00, -1.9071718e+00, -1.2346470e+00,  1.2859275e+00,\n",
      "         3.9383078e+00],\n",
      "       [-2.0034237e+00, -1.7378058e+00, -1.1613771e+00,  1.1813126e+00,\n",
      "         3.8031750e+00],\n",
      "       [-2.0188909e+00, -1.5948954e+00, -8.1679851e-01,  1.1033680e+00,\n",
      "         2.7013764e+00],\n",
      "       [-2.2495313e+00, -1.4651140e+00, -7.2683156e-01,  9.4583118e-01,\n",
      "         2.9315026e+00],\n",
      "       [-2.1262975e+00, -1.3847952e+00, -6.9362360e-01,  7.5194669e-01,\n",
      "         2.5189459e+00],\n",
      "       [-2.6083028e+00, -1.3561053e+00, -3.8388312e-01,  9.3942261e-01,\n",
      "         2.6728640e+00],\n",
      "       [-2.0374110e+00, -1.2967664e+00, -8.5196501e-01,  4.7907993e-01,\n",
      "         2.7502880e+00],\n",
      "       [-2.0773883e+00, -1.3158768e+00, -4.3321383e-01,  8.1189162e-01,\n",
      "         1.7236886e+00],\n",
      "       [-1.9268737e+00, -1.2797152e+00, -8.1840062e-01,  4.9123514e-01,\n",
      "         2.4415059e+00],\n",
      "       [-1.7878338e+00, -1.7067251e+00, -1.2677974e+00,  9.9554563e-01,\n",
      "         3.5682406e+00],\n",
      "       [-1.9430618e+00, -1.8541567e+00, -1.0904024e+00,  1.5028228e+00,\n",
      "         3.3780341e+00],\n",
      "       [-2.5442817e+00, -1.4867412e+00, -3.4709209e-01,  9.9244511e-01,\n",
      "         2.5666213e+00],\n",
      "       [-2.8251693e+00, -1.3021864e+00,  6.2830821e-02,  9.5762914e-01,\n",
      "         1.9572234e+00],\n",
      "       [-2.3834469e+00, -1.8552930e+00, -6.2403744e-01,  1.6565704e+00,\n",
      "         3.0584803e+00],\n",
      "       [-1.6098926e+00, -1.8325869e+00, -1.4987378e+00,  1.1589563e+00,\n",
      "         4.2232637e+00],\n",
      "       [-2.0662401e+00, -1.0946732e+00, -4.3189019e-01,  4.7484317e-01,\n",
      "         1.7637788e+00],\n",
      "       [-2.2252922e+00, -1.3032663e+00, -6.3584346e-01,  7.1137071e-01,\n",
      "         2.5701826e+00],\n",
      "       [-2.2917709e+00, -1.6946285e+00, -8.0678624e-01,  1.4076271e+00,\n",
      "         3.2489111e+00],\n",
      "       [-2.1219037e+00, -1.7687613e+00, -1.0168314e+00,  1.2326360e+00,\n",
      "         3.6907947e+00],\n",
      "       [-2.3920958e+00, -1.7636340e+00, -8.0640143e-01,  1.5943209e+00,\n",
      "         3.5458350e+00],\n",
      "       [-2.1462848e+00, -1.6080573e+00, -7.2346944e-01,  1.0879757e+00,\n",
      "         2.7574823e+00],\n",
      "       [-2.4930027e+00, -1.9009452e+00, -4.8076010e-01,  1.8715608e+00,\n",
      "         2.7589593e+00],\n",
      "       [-2.8925765e+00, -2.1862984e+00, -1.2526894e-01,  2.5456033e+00,\n",
      "         2.9974036e+00],\n",
      "       [-2.4601986e+00, -1.4675658e+00, -2.3000784e-01,  1.4183435e+00,\n",
      "         1.8831758e+00],\n",
      "       [-2.8430030e+00, -2.1403594e+00, -2.2684054e-01,  2.5487828e+00,\n",
      "         3.1475575e+00],\n",
      "       [-2.4477651e+00, -1.6199591e+00, -5.1821178e-01,  1.3387144e+00,\n",
      "         2.7721362e+00],\n",
      "       [-2.6424689e+00, -1.9209583e+00, -2.5222790e-01,  2.1159394e+00,\n",
      "         2.4103618e+00],\n",
      "       [-2.3585782e+00, -1.2047386e+00, -3.4184903e-01,  8.0170548e-01,\n",
      "         2.0151658e+00],\n",
      "       [-2.6286304e+00, -1.9629476e+00, -3.5497731e-01,  2.1233559e+00,\n",
      "         2.7348855e+00],\n",
      "       [-2.8658433e+00, -1.4111266e+00, -1.0047376e-02,  1.5692273e+00,\n",
      "         1.7523843e+00],\n",
      "       [-2.7259519e+00, -1.6122892e+00, -2.0219791e-01,  1.7133851e+00,\n",
      "         2.1327269e+00],\n",
      "       [-2.6460969e+00, -1.8902171e+00, -3.1141275e-01,  1.9991853e+00,\n",
      "         2.5435619e+00],\n",
      "       [-2.5996110e+00, -1.6341467e+00, -2.6613963e-01,  1.6194551e+00,\n",
      "         2.1644268e+00],\n",
      "       [-2.0788910e+00, -1.7948232e+00, -1.0911641e+00,  1.2124021e+00,\n",
      "         3.8841639e+00],\n",
      "       [-2.4968348e+00, -1.6199846e+00, -4.0699261e-01,  1.4735326e+00,\n",
      "         2.3706770e+00],\n",
      "       [-2.4008553e+00, -1.7751266e+00, -5.6155705e-01,  1.5995471e+00,\n",
      "         2.9115806e+00],\n",
      "       [-2.4509485e+00, -1.9206169e+00, -5.6352574e-01,  1.9438564e+00,\n",
      "         3.0741651e+00],\n",
      "       [-1.9993980e+00, -9.9037349e-01, -3.6702293e-01,  4.5578462e-01,\n",
      "         1.2767749e+00],\n",
      "       [ 3.2033628e-01,  5.2669245e-01,  1.7520358e-01, -1.1060770e+00,\n",
      "        -1.9212115e+00],\n",
      "       [-2.3066809e+00, -1.3356538e+00, -4.9616104e-01,  9.4015992e-01,\n",
      "         2.2703581e+00],\n",
      "       [-2.4414570e+00, -1.6010749e+00, -4.4886565e-01,  1.4530776e+00,\n",
      "         2.5032425e+00],\n",
      "       [-2.1791303e+00, -1.7379782e+00, -7.4119043e-01,  1.5249913e+00,\n",
      "         2.7789965e+00],\n",
      "       [-1.4290872e+00, -1.2122331e+00, -5.5717582e-01,  4.4574121e-01,\n",
      "         1.0867523e+00],\n",
      "       [-2.3571260e+00, -1.3967826e+00, -6.8767041e-01,  8.1017679e-01,\n",
      "         2.9801612e+00],\n",
      "       [-2.3221722e+00, -1.3976116e+00, -3.1750757e-01,  1.0782704e+00,\n",
      "         1.8840485e+00]], dtype=float32)), label_ids=None, metrics={'test_runtime': 46.1599, 'test_samples_per_second': 2.166, 'test_steps_per_second': 0.087})\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "294ecb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_model\n",
      "Configuration saved in test_model\\config.json\n",
      "Model weights saved in test_model\\pytorch_model.bin\n",
      "tokenizer config file saved in test_model\\tokenizer_config.json\n",
      "Special tokens file saved in test_model\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "366251d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file test_model\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"test_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"1\": \"star 1\",\n",
      "    \"2\": \"star 2\",\n",
      "    \"3\": \"star 3\",\n",
      "    \"4\": \"star 4\",\n",
      "    \"5\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": \"1\",\n",
      "    \"star 2\": \"2\",\n",
      "    \"star 3\": \"3\",\n",
      "    \"star 4\": \"4\",\n",
      "    \"star 5\": \"5\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file test_model\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"test_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"1\": \"star 1\",\n",
      "    \"2\": \"star 2\",\n",
      "    \"3\": \"star 3\",\n",
      "    \"4\": \"star 4\",\n",
      "    \"5\": \"star 5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"star 1\": \"1\",\n",
      "    \"star 2\": \"2\",\n",
      "    \"star 3\": \"3\",\n",
      "    \"star 4\": \"4\",\n",
      "    \"star 5\": \"5\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file test_model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at test_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task= 'sentiment-analysis', \n",
    "                      model= \"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8577a945",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable BertForSequenceClassification object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [153], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result, model_outputs, predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable BertForSequenceClassification object"
     ]
    }
   ],
   "source": [
    "result, model_outputs, predictions = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6e126207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'star 1', 'score': 0.41573843359947205},\n",
       " {'label': 'star 3', 'score': 0.3553646206855774}]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\n",
    "        \"一進到店內，就聞到臭味，很噁心\",\n",
    "        \"麵的軟硬度剛好，湯也不會太鹹，不推薦大家來\"\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st",
   "language": "python",
   "name": "st"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
